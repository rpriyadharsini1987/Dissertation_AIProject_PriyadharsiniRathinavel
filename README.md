
#  Testing Framework

A unified framework for analyzing, evaluating, and visualizing student code attempts, LLM-generated questions/answers, and compliance metrics for educational hackathons and coding assessments.

---

## ğŸ“¦ Project Structure

```
â”œâ”€â”€ answer_comparison.py                # Compare LLM and student answers
â”œâ”€â”€ data_extractor.py                   # Extract and preprocess main dataset
â”œâ”€â”€ extract_formatted_data.py           # Format and extract data for analysis
â”œâ”€â”€ prompt_handler.py                   # Prompt management for LLMs
â”œâ”€â”€ topic_question_student_flag.py      # Analyze student question relevance flags
â”œâ”€â”€ topics.py                           # Extract code topics from submissions
â”œâ”€â”€ compliance/
â”‚   â””â”€â”€ compliance.py                   # Compliance checks for answers/questions
â”œâ”€â”€ config/
â”‚   â””â”€â”€ .env                            # Environment variables (LLM provider, etc.)
â”œâ”€â”€ deepeval/
â”‚   â””â”€â”€ data_formatter.py               # Data formatting for DeepEval metrics
â”œâ”€â”€ frontend_bot/
â”‚   â””â”€â”€ bot_ui.py                       # (Experimental) Frontend bot UI
â”œâ”€â”€ input_data/
â”‚   â”œâ”€â”€ bulk_data/                      # Raw input datasets
â”‚   â””â”€â”€ minimal_data/                   # Processed/filtered datasets
â”œâ”€â”€ json_reports/                       # Output reports (JSON)
â”œâ”€â”€ not_required/
â”‚   â””â”€â”€ extract.py                      # (Deprecated) Extraction scripts
â”œâ”€â”€ prompts/                            # Prompt templates for LLMs
â”œâ”€â”€ ragas/                              # Ragas metric evaluation scripts
â”œâ”€â”€ streamlit/
â”‚   â””â”€â”€ streamlit_combined_dashboard.py # Streamlit dashboard for visualization
â”œâ”€â”€ requirements.txt                    # Python dependencies
â””â”€â”€ README.md                           # Project documentation
```

---

## ğŸš€ Quickstart

### 1. Install Python 3.11

```sh
brew install python@3.11
```

### 2. Create and Activate Virtual Environment

```sh
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Dependencies

```sh
pip install -r requirements.txt
pip install openai
python -m spacy download en_core_web_sm
pip install --upgrade langchain_openai
```

### 4. Prepare Data

- **Extract and preprocess data:**
```sh
python3 data_extractor.py
```
- **Extract code topics:**
```sh
python3 topics.py
```
- **Analyze student question flags:**
```sh
python3 topic_question_student_flag.py
```
- **Compare LLM and student answers:**
```sh
python3 answer_comparison.py
```
- **Compliance checks:**
```sh
python3 compliance/compliance.py
```
- **Evaluate Ragas metrics:**
```sh
python3 ragas/evaluate_ragas.py
```
- **Validate Ragas metrics:**
```sh
python3 ragas/validate_ragas_metrics.py
```

### 5. Run the Dashboard

```sh
streamlit run streamlit/streamlit_combined_dashboard.py
```

---

## ğŸ“Š Features

- **Data Extraction & Formatting:** Scripts to clean, filter, and format student code and question/answer data.
- **Topic Extraction:** Automatic inference of code topics from student submissions.
- **LLM & Student Answer Comparison:** Analyze and visualize how student answers compare to LLM-generated answers.
- **Compliance Checking:** Ensure answers/questions meet compliance requirements.
- **Metric Evaluation:** Ragas and custom metrics for answer/question quality.
- **Interactive Dashboard:** Streamlit-based dashboard for unified visualization and analysis.

---

## âš™ï¸ Configuration

- Set your LLM provider and other environment variables in `config/.env`:
  ```
  LLM_PROVIDER=gemini  # or openai, etc.
  ```

---

## ğŸ“š Requirements

See [`requirements.txt`](requirements.txt) for all dependencies, including:
- `streamlit`
- `python-dotenv`
- `spacy`
- `langchain_openai`
- `ragas`
- `deepeval`
- and others

---

## ğŸ“ Data

- Place your input datasets in `input_data/bulk_data/` or `input_data/minimal_data/`.
- Output reports are generated in `json_reports/`.

---

## ğŸ“ Notes

- Some scripts and folders (e.g., `not_required/`) are deprecated or experimental.
- Prompts for LLMs are stored in the `prompts/` directory.
- The dashboard supports multiple LLM providers (set in `.env`).

---

## ğŸ–¥ï¸ Dashboard Overview

The Streamlit dashboard provides three main tabs:
1. **Code Topics Report:** Review code topics and generated questions for each student attempt.
2. **Switch Model Report:** Analyze question relevance and flagging accuracy.
3. **LLM vs Student Answer Match:** Compare LLM and student answers, with verdicts for each question.

---

## ğŸ“„ License

MIT License (add your license here if different).

---


